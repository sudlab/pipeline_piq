"""===========================
Pipeline template
===========================

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.yml` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_piq.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_genesets`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
import sys
import os
import CGATCore.IOTools as IOTools
import glob
import tempfile
import pipelinePIQ
from ruffus import transform, regex, suffix, follows, mkdir, split, formatter, collate
from ruffus.combinatorics import product


from CGATCore import Pipeline as P

# load options from the config file
PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])

PARAMS["COMMONSCRIPT"] = os.path.join(PARAMS["PIQ_PATH"], "common.r")


# Based on Copied from Motif_tools.motif_db_formatting_py3
# 
# Inputs:
#     -infile: A jaspar motif format file. Such as:
#        /shared/sudlab1/General/apps/bio/PIQ_human/pwms/jasparfix.txt
#
# >MA0001.1;AGL3
# A  [ 0  3 79 40 66 48 65 11 65  0 ]
# C  [94 75  4  3  1  2  5  2  3  3 ]
# G  [ 1  0  3  4  1  0  5  3 28 88 ]
# T  [ 2 19 11 50 29 47 22 81  1  6 ]
# >MA0002.1;RUNX1
# A  [10 12  4  1  2  2  0  0  0  8 13 ]
# C  [ 2  2  7  1  0  8  0  0  1  2  2 ]
# G  [ 3  1  1  0 23  0 26 26  0  0  4 ]
# T  [11 11 14 24  1 16  0  0 25 16  7 ]
#
# -number of motifs: Counts the number of lines beginning with ">"
#
# Outputs:
#     The number of motifs
def countMotifs(infile):
    
    num_motifs = 0
    
    with IOTools.open_file(infile, "r") as reader:
        
        for line in reader:

            # New motif
            if line.startswith(">"):
                
                num_motifs += 1
                
        return num_motifs
                
    reader.close()


# ---------------------------------------------------
# Specific pipeline tasks
@follows(mkdir("motif.matchs"))
@split(os.path.join(PARAMS["PIQ_PATH"], "pwms/jasparfix.txt"),
       ["motif.matchs/%i.pwmout.RData" % (i+1) for i in range(countMotifs(os.path.join(PARAMS["PIQ_PATH"], "pwms/jasparfix.txt")))])
def process_pwms(infile, outfiles):
    '''Generate the PWM hits across genome. Checks to see which outfiles have already been created and
    are newer than the infile and adds the rest to process'''
    
    # Go through the outfiles and get the id of the files which are no created (normal strand AND rc)
    # Also files which are newer than the infile
    ids_remaining_to_process = []
    
    for outfile in outfiles:
        
        # Get the rc file 1000.pwmout.rc.RData - 1000.pwmout.RData
        rc_outfile = P.snip(outfile, ".RData") + ".rc.RData"
        
        # Get the id of the file
        id_outfile = P.snip(os.path.basename(outfile), ".pwmout.RData")
        
        
        # If any of the files doesn't exist
        if ( (not os.path.isfile(outfile)) or (not os.path.isfile(rc_outfile)) ):
            
            ids_remaining_to_process.append(id_outfile)
        
        # If any of the files is not newer than the infile
        # Check only if both files already exist    
        elif( (not(os.path.getctime(infile) < os.path.getctime(outfile))) or 
            (not(os.path.getctime(infile) < os.path.getctime(rc_outfile))) ):
            
            ids_remaining_to_process.append(id_outfile)
           
            
    # Get the temp dir        
    tmp_dir = PARAMS["shared_tmpdir"]
    
    statements = []
    statement = []
    match_script = os.path.join(PARAMS["PIQ_PATH"], "pwmmatch.exact.r")
    out_dir = os.path.dirname(os.path.abspath(outfiles[0]))
    job_memory = "8G"
    
    # % gets substituted with %locals, %% gets substituted when submitting
    statement_template = "Rscript %%(match_script)s %%(COMMONSCRIPT)s %%(infile)s %(i)i %(output_temp_dir)s && mv %(output_temp_dir)s/%(i)i.pwmout.*RData %%(out_dir)s/"
    
    
    # Variable to control the number of ids processed
    id_num_counter = 1
    
    for id in ids_remaining_to_process:
        
        # Convert the id to int
        i = int(id)
        
        # Everytime we begin a new chunk of statements we create a new temporal dir 
        # append cd PIQ_PATH
        if((id_num_counter-1)%PARAMS["chunk_size"] == 0):
            # Create a temporary dir
            output_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
            statement.append("cd %(PIQ_PATH)s && " + (statement_template % locals()))
        else:
            statement.append(statement_template % locals())
        
        # After we add the individual statement we concatenate statements in chunks of selected size     
        if len(statement) == PARAMS["chunk_size"]:
            # Remove the temporal directory
            statement.append("rm -rf %(output_temp_dir)s" % locals())
            
            statements.append(" && ".join(statement))
            statement = []
        
        # Update the number of ids processed
        id_num_counter += 1
        
    # Append the remaining statements
    
    # Remove the temporal directory
    statement.append("rm -rf %(output_temp_dir)s" % locals())
    
    statements.append(" && ".join(statement))
    
    
    P.run(statements, job_condaenv=PARAMS["piq_condaenv"])
    
    

@follows(mkdir("processed_bams.dir"))    
@transform("*.bam", formatter(), "processed_bams.dir/{basename[0]}.RData")
def process_bam(infile, outfile):
    
    # Get the temp dir        
    tmp_dir = PARAMS["shared_tmpdir"]
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_outfile = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    

    infile = os.path.abspath(infile)
    outfile = os.path.abspath(outfile)
    bam2rdata_script = os.path.join(PARAMS["PIQ_PATH"], "bam2rdata.r")
    statement = '''cd %(PIQ_PATH)s &&
                   Rscript %(bam2rdata_script)s %(COMMONSCRIPT)s %(temp_outfile)s %(infile)s && 
                   mv %(temp_outfile)s %(outfile)s'''
    job_memory = "64G"
    
    P.run(statement, job_condaenv=PARAMS["piq_condaenv"])


@follows(mkdir("calls.dir"))
@product(process_bam,
         formatter(),
         process_pwms,
         formatter("motif.matchs/(?P<PWM>.+).pwmout.RData"),
         "calls.dir/{basename[0][0]}/{PWM[1][0]}.done")
def call_matches(infiles, outfile):

    bamfile, pwm = infiles
    
    # Get the temp dir        
    tmp_dir = PARAMS["shared_tmpdir"]
    
    output_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
    
    bamfile = os.path.abspath(bamfile)
    matchesdir = os.path.abspath(os.path.dirname(pwm))
    pwm_no = P.snip(os.path.basename(pwm), ".pwmout.RData")
    outfile = os.path.abspath(outfile)  
    pertf_script = os.path.join(PARAMS["PIQ_PATH"], "pertf.r")
    outdir = os.path.dirname(os.path.abspath(outfile))
    statement = '''cd %(PIQ_PATH)s &&
                   Rscript %(pertf_script)s 
                            %(COMMONSCRIPT)s 
                            %(matchesdir)s/  
                            %(output_temp_dir)s 
                            %(outdir)s 
                            %(bamfile)s
                            %(pwm_no)s &&
                  rm -rf %(output_temp_dir)s &&
                  touch %(outfile)s'''

    # Memory usage depends on input size

    size = os.path.getsize(pwm)
    size = size/(1024.0*1024.0)

    bamsize = os.path.getsize(bamfile)
    bamsize = bamsize/(1024.0*1024.0)

    if bamsize > 700:
        size += 15

    if size > 60:
        job_memory = "128G"
    elif size > 38:
        job_memory = "64G"    
    elif size > 22:
        job_memory = "32G"
    elif size > 15:
        job_memory = "16G"
    elif size > 5:
        job_memory = "8G"
    else:
        job_memory = "4G"

    P.run(statement, job_condaenv = PARAMS["piq_condaenv"])
    
    


@follows(call_matches,
         mkdir("calls_filtered.dir"))
@collate("calls.dir/*/*-calls.*",
           formatter("calls.dir/(?P<SAMPLE>.+)/(?P<PWM>.+?)(\.RC|)-calls\..*"),
           "calls_filtered.dir/{SAMPLE[0]}/{PWM[0]}-calls.sign.bed.gz",
           "{SAMPLE[0]}",
           "{PWM[0]}")
def filter_matches(infiles, outfile, sample, pwm):
    
   
    if(len(infiles) != 6):
        raise Exception("All the call files are not produced")
    
    # If the directory for the sample doesn't exist, create it
    if not os.path.exists(os.path.dirname(outfile)):
        os.makedirs(os.path.dirname(outfile))

    
    # Get the different files
    calls_all_file=""
    RC_calls_all_file=""
    calls_sign_file=""
    RC_calls_sign_file=""
    
    for infile in infiles:
        if infile.endswith("-calls.all.bed"):
            if infile.endswith(".RC-calls.all.bed"):
                RC_calls_all_file=infile
            else:
                calls_all_file=infile
        
        elif infile.endswith("-calls.csv"):
            if infile.endswith(".RC-calls.csv"):
                RC_calls_sign_file=infile
            else:
                calls_sign_file=infile
    
    # Get the temp dir        
    tmp_dir = PARAMS["shared_tmpdir"]
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    
    # Processing of the files containing all calls
    temp_file_all_calls = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    temp_file_all_calls_RC = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name

    # Processing of the
    temp_file_calls_sign = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    temp_file_calls_sign_RC = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    
            
    # The first step is to remove the header from both files and filter individually the
    # significant motifs
    # Output the rest of the file adding a numeric count starting at 1
    statement = '''cat %(calls_all_file)s | tail -n +2 | awk 'BEGIN {FS ="\\t"} {printf("%%s\\t%%s\\n", NR, $0)}' | gzip > %(temp_file_all_calls)s  &&
        
    cat %(RC_calls_all_file)s | tail -n +2 | awk 'BEGIN {FS ="\\t"} {printf("%%s\\t%%s\\n", NR, $0)}' | gzip > %(temp_file_all_calls_RC)s  &&
    
    cat %(calls_sign_file)s | tail -n +2 | sed -e 's/,/\\t/g' | cut -f 1 | sed -e 's/"//g' | gzip > %(temp_file_calls_sign)s  &&
        
    cat %(RC_calls_sign_file)s | tail -n +2 | sed -e 's/,/\\t/g' | cut -f 1 | sed -e 's/"//g' | gzip > %(temp_file_calls_sign_RC)s '''
    
    P.run(statement)
    
    
    # Processing of the bed files to select significant calls
    temp_file_calls_sign_bed = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    temp_file_calls_sign_bed_RC = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    # Now merge the tables to select only significant calls in bed format
    pipelinePIQ.filterSignCalls(temp_file_all_calls,
                    temp_file_calls_sign,
                    temp_file_calls_sign_bed)
    
    pipelinePIQ.filterSignCalls(temp_file_all_calls_RC,
                    temp_file_calls_sign_RC,
                    temp_file_calls_sign_bed_RC)
    
    
    # Create temp output
    temp_output = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    
    
    # Now concatenate both bed files and sort by chr and start
    # Delete all temporal files 

    statement = '''zcat %(temp_file_calls_sign_bed)s %(temp_file_calls_sign_bed_RC)s | sort -k 1,1 -k2,2n | gzip > %(temp_output)s && 
    rm %(temp_file_all_calls)s %(temp_file_all_calls_RC)s %(temp_file_calls_sign)s %(temp_file_calls_sign_RC)s %(temp_file_calls_sign_bed)s %(temp_file_calls_sign_bed_RC)s && 
    mv %(temp_output)s %(outfile)s '''
    
    P.run(statement)
    

   
       



# ---------------------------------------------------
# Generic pipeline tasks
@follows(filter_matches)
def full():
    pass


def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))    
